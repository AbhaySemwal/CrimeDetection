{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from multiprocessing import Process\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SKIP = 2\n",
    "FRAME_SIZE = (150,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir Data\n",
    "# !mkdir -p ./Data/Video/Violent\n",
    "# !mkdir -p ./Data/Video/NonViolent\n",
    "# !cp -a ./A-Dataset-for-Automatic-Violence-Detection-in-Videos/violence-detection-dataset/violent/cam1/. ./Data/Video/Violent/\n",
    "# !cp -a ./A-Dataset-for-Automatic-Violence-Detection-in-Videos/violence-detection-dataset/non-violent/cam1/. ./Data/Video/NonViolent/\n",
    "# clear_output()\n",
    "# !mkdir -p ./Data/Training/V\n",
    "# !mkdir -p ./Data/Training/NV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, output_path, frame_size=(150, 150), frame_skip=2):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_skip == 0:\n",
    "            frame = cv2.resize(frame, frame_size)\n",
    "            frame_filename = os.path.join(output_path, f\"{os.path.splitext(os.path.basename(video_path))[0]}_frame_{count:04d}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "\n",
    "def thread_1():\n",
    "    for i in range(60):\n",
    "        video_path = f\"./Data/Video/Violent/{i+1}.mp4\"\n",
    "        output_path = \"./Data/Training/V\"\n",
    "        print(f\"Processing Violent Vid-{i}\")\n",
    "        extract_frames(video_path, output_path, frame_size=FRAME_SIZE, frame_skip=FRAME_SKIP)\n",
    "    print(\"Violent Extracted\")\n",
    "\n",
    "def thread_2():\n",
    "    for i in range(60):\n",
    "        video_path = f\"./Data/Video/NonViolent/{i+1}.mp4\"\n",
    "        output_path = \"./Data/Training/NV\"\n",
    "        print(f\"Processing NonViolent Vid-{i}\")\n",
    "        extract_frames(video_path, output_path, frame_size=FRAME_SIZE, frame_skip=FRAME_SKIP)\n",
    "    print(\"Non-Violent Extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Violent Vid-0\n",
      "Processing NonViolent Vid-0\n",
      "Processing Violent Vid-1\n",
      "Processing NonViolent Vid-1\n",
      "Processing NonViolent Vid-2\n",
      "Processing Violent Vid-2\n",
      "Processing NonViolent Vid-3\n",
      "Processing Violent Vid-3\n",
      "Processing NonViolent Vid-4\n",
      "Processing Violent Vid-4\n",
      "Processing NonViolent Vid-5\n",
      "Processing Violent Vid-5\n",
      "Processing NonViolent Vid-6\n",
      "Processing Violent Vid-6\n",
      "Processing NonViolent Vid-7\n",
      "Processing Violent Vid-7\n",
      "Processing NonViolent Vid-8\n",
      "Processing NonViolent Vid-9\n",
      "Processing Violent Vid-8\n",
      "Processing NonViolent Vid-10\n",
      "Processing Violent Vid-9\n",
      "Processing NonViolent Vid-11\n",
      "Processing Violent Vid-10\n",
      "Processing NonViolent Vid-12\n",
      "Processing Violent Vid-11\n",
      "Processing Violent Vid-12\n",
      "Processing NonViolent Vid-13\n",
      "Processing NonViolent Vid-14\n",
      "Processing Violent Vid-13\n",
      "Processing Violent Vid-14\n",
      "Processing NonViolent Vid-15\n",
      "Processing Violent Vid-15\n",
      "Processing NonViolent Vid-16\n",
      "Processing Violent Vid-16\n",
      "Processing Violent Vid-17\n",
      "Processing NonViolent Vid-17\n",
      "Processing Violent Vid-18\n",
      "Processing NonViolent Vid-18\n",
      "Processing NonViolent Vid-19\n",
      "Processing Violent Vid-19\n",
      "Processing NonViolent Vid-20\n",
      "Processing Violent Vid-20\n",
      "Processing NonViolent Vid-21\n",
      "Processing Violent Vid-21\n",
      "Processing NonViolent Vid-22\n",
      "Processing Violent Vid-22\n",
      "Processing NonViolent Vid-23\n",
      "Processing Violent Vid-23\n",
      "Processing NonViolent Vid-24\n",
      "Processing Violent Vid-24\n",
      "Processing NonViolent Vid-25\n",
      "Processing Violent Vid-25\n",
      "Processing NonViolent Vid-26\n",
      "Processing Violent Vid-26\n",
      "Processing NonViolent Vid-27\n",
      "Processing Violent Vid-27\n",
      "Processing NonViolent Vid-28\n",
      "Processing Violent Vid-28\n",
      "Processing NonViolent Vid-29\n",
      "Processing Violent Vid-29\n",
      "Processing NonViolent Vid-30\n",
      "Processing Violent Vid-30\n",
      "Processing Violent Vid-31\n",
      "Processing NonViolent Vid-31\n",
      "Processing Violent Vid-32\n",
      "Processing NonViolent Vid-32\n",
      "Processing Violent Vid-33\n",
      "Processing NonViolent Vid-33\n",
      "Processing Violent Vid-34\n",
      "Processing NonViolent Vid-34\n",
      "Processing Violent Vid-35\n",
      "Processing NonViolent Vid-35\n",
      "Processing NonViolent Vid-36\n",
      "Processing Violent Vid-36\n",
      "Processing NonViolent Vid-37\n",
      "Processing NonViolent Vid-38\n",
      "Processing Violent Vid-37\n",
      "Processing NonViolent Vid-39\n",
      "Processing Violent Vid-38\n",
      "Processing NonViolent Vid-40\n",
      "Processing Violent Vid-39\n",
      "Processing NonViolent Vid-41\n",
      "Processing Violent Vid-40\n",
      "Processing NonViolent Vid-42\n",
      "Processing Violent Vid-41\n",
      "Processing NonViolent Vid-43\n",
      "Processing NonViolent Vid-44\n",
      "Processing Violent Vid-42\n",
      "Processing NonViolent Vid-45\n",
      "Processing NonViolent Vid-46\n",
      "Processing Violent Vid-43\n",
      "Processing NonViolent Vid-47\n",
      "Processing Violent Vid-44\n",
      "Processing NonViolent Vid-48\n",
      "Processing Violent Vid-45\n",
      "Processing NonViolent Vid-49\n",
      "Processing Violent Vid-46\n",
      "Processing NonViolent Vid-50\n",
      "Processing Violent Vid-47\n",
      "Processing NonViolent Vid-51\n",
      "Processing Violent Vid-48\n",
      "Processing NonViolent Vid-52\n",
      "Processing NonViolent Vid-53\n",
      "Processing Violent Vid-49\n",
      "Processing NonViolent Vid-54\n",
      "Processing Violent Vid-50\n",
      "Processing NonViolent Vid-55\n",
      "Processing Violent Vid-51\n",
      "Processing NonViolent Vid-56\n",
      "Processing Violent Vid-52\n",
      "Processing NonViolent Vid-57\n",
      "Processing Violent Vid-53\n",
      "Processing NonViolent Vid-58\n",
      "Processing NonViolent Vid-59\n",
      "Processing Violent Vid-54\n",
      "Non-Violent Extracted\n",
      "Processing Violent Vid-55\n",
      "Processing Violent Vid-56\n",
      "Processing Violent Vid-57\n",
      "Processing Violent Vid-58\n",
      "Processing Violent Vid-59\n",
      "Violent Extracted\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "t1 = Process(target=thread_1, args=())\n",
    "t2 = Process(target=thread_2, args=())\n",
    "\n",
    "t1.start() \n",
    "t2.start()\n",
    "\n",
    "t1.join()\n",
    "t2.join()\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='./Data'\n",
    "train_dir=os.path.join(base_dir,'Training')\n",
    "train_violent_dir =os.path.join(train_dir, 'V' )\n",
    "train_nonviolent_dir=os.path.join(train_dir,'NV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen= ImageDataGenerator(rescale=1./255, rotation_range=40,width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2,horizontal_flip=True, fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10280 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_dir,color_mode=\"rgb\", target_size = FRAME_SIZE,batch_size=20,classes=['NV','V'], class_mode='binary', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= tf.keras.models.Sequential([\n",
    "       tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)),\n",
    "       tf.keras.layers.MaxPooling2D(2,2),\n",
    "       tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
    "       tf.keras.layers.MaxPooling2D(2,2),\n",
    "       tf.keras.layers.Conv2D(128,(3,3),activation='relu'),\n",
    "       tf.keras.layers.MaxPooling2D(2,2),\n",
    "       tf.keras.layers.Conv2D(128,(3,3),activation='relu'),\n",
    "       tf.keras.layers.MaxPooling2D(2,2),\n",
    "       tf.keras.layers.Dropout(0.5),\n",
    "       tf.keras.layers.Flatten(),\n",
    "       tf.keras.layers.Dense(512, activation='relu'),\n",
    "       tf.keras.layers.Dense(1,activation ='sigmoid')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 702ms/step - accuracy: 0.5422 - loss: 0.6913\n",
      "Epoch 2/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 710ms/step - accuracy: 0.5446 - loss: 0.6928\n",
      "Epoch 3/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 651ms/step - accuracy: 0.5672 - loss: 0.6920\n",
      "Epoch 4/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 679ms/step - accuracy: 0.5971 - loss: 0.6787\n",
      "Epoch 5/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 643ms/step - accuracy: 0.6448 - loss: 0.6550\n",
      "Epoch 6/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 697ms/step - accuracy: 0.7724 - loss: 0.5309\n",
      "Epoch 7/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 643ms/step - accuracy: 0.8208 - loss: 0.4279\n",
      "Epoch 8/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 689ms/step - accuracy: 0.8130 - loss: 0.4289\n",
      "Epoch 9/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 688ms/step - accuracy: 0.7451 - loss: 0.5074\n",
      "Epoch 10/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 661ms/step - accuracy: 0.8011 - loss: 0.4317\n",
      "Epoch 11/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 187ms/step - accuracy: 0.8255 - loss: 0.4128\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 23:14:56.522118: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 1s/step - accuracy: 0.8379 - loss: 0.3815\n",
      "Epoch 13/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 646ms/step - accuracy: 0.7964 - loss: 0.4256\n",
      "Epoch 14/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 664ms/step - accuracy: 0.8570 - loss: 0.3399\n",
      "Epoch 15/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 647ms/step - accuracy: 0.7832 - loss: 0.4646\n",
      "Epoch 16/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 640ms/step - accuracy: 0.8380 - loss: 0.3798\n",
      "Epoch 17/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 693ms/step - accuracy: 0.8621 - loss: 0.3454\n",
      "Epoch 18/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 679ms/step - accuracy: 0.8333 - loss: 0.3653\n",
      "Epoch 19/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 655ms/step - accuracy: 0.8384 - loss: 0.3546\n",
      "Epoch 20/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 662ms/step - accuracy: 0.8522 - loss: 0.3425\n",
      "Epoch 21/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 734ms/step - accuracy: 0.8743 - loss: 0.3189\n",
      "Epoch 22/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.8392 - loss: 0.3320 \n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 23:21:01.011384: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.8526 - loss: 0.3591\n",
      "Epoch 24/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 642ms/step - accuracy: 0.8706 - loss: 0.3182\n",
      "Epoch 25/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 702ms/step - accuracy: 0.8569 - loss: 0.3305\n",
      "Epoch 26/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 783ms/step - accuracy: 0.8599 - loss: 0.3413\n",
      "Epoch 27/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 667ms/step - accuracy: 0.8765 - loss: 0.2829\n",
      "Epoch 28/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 651ms/step - accuracy: 0.8908 - loss: 0.2859\n",
      "Epoch 29/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 646ms/step - accuracy: 0.8789 - loss: 0.2840\n",
      "Epoch 30/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 669ms/step - accuracy: 0.8446 - loss: 0.3591\n"
     ]
    }
   ],
   "source": [
    "model1=model.fit(train_generator,steps_per_epoch=50, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./1717437385.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "export_path_keras = \"./{}.h5\".format(int(t))\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
