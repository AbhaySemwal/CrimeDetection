{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 16:22:11.465766: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-01 16:22:11.491373: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-01 16:22:11.634332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-01 16:22:13.035904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from multiprocessing import Process\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SKIP = 2\n",
    "FRAME_SIZE = (150,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r A-Dataset-for-Automatic-Violence-Detection-in-Videos/\n",
    "# !git clone https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r Data\n",
    "# !mkdir Data\n",
    "# !mkdir -p ./Data/Video/Violent\n",
    "# !mkdir -p ./Data/Video/NonViolent\n",
    "# !cp -a ./A-Dataset-for-Automatic-Violence-Detection-in-Videos/violence-detection-dataset/violent/cam1/. ./Data/Video/Violent/\n",
    "# !cp -a ./A-Dataset-for-Automatic-Violence-Detection-in-Videos/violence-detection-dataset/non-violent/cam1/. ./Data/Video/NonViolent/\n",
    "# clear_output()\n",
    "# !mkdir -p ./Data/Training/V\n",
    "# !mkdir -p ./Data/Training/NV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, output_path, frame_size=(150, 150), frame_skip=2):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_skip == 0:\n",
    "            frame = cv2.resize(frame, frame_size)\n",
    "            frame_filename = os.path.join(output_path, f\"{os.path.splitext(os.path.basename(video_path))[0]}_frame_{count:04d}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "\n",
    "def thread_1():\n",
    "    for i in range(60):\n",
    "        video_path = f\"./Data/Video/Violent/{i+1}.mp4\"\n",
    "        output_path = \"./Data/Training/V\"\n",
    "        print(f\"Processing Violent Vid-{i}\")\n",
    "        extract_frames(video_path, output_path, frame_size=FRAME_SIZE, frame_skip=FRAME_SKIP)\n",
    "    print(\"Violent Extracted\")\n",
    "\n",
    "def thread_2():\n",
    "    for i in range(60):\n",
    "        video_path = f\"./Data/Video/NonViolent/{i+1}.mp4\"\n",
    "        output_path = \"./Data/Training/NV\"\n",
    "        print(f\"Processing NonViolent Vid-{i}\")\n",
    "        extract_frames(video_path, output_path, frame_size=FRAME_SIZE, frame_skip=FRAME_SKIP)\n",
    "    print(\"Non-Violent Extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Violent Vid-0\n",
      "Processing NonViolent Vid-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Violent Vid-1\n",
      "Processing NonViolent Vid-1\n",
      "Processing NonViolent Vid-2\n",
      "Processing Violent Vid-2\n",
      "Processing NonViolent Vid-3\n",
      "Processing Violent Vid-3\n",
      "Processing NonViolent Vid-4\n",
      "Processing Violent Vid-4\n",
      "Processing Violent Vid-5\n",
      "Processing NonViolent Vid-5\n",
      "Processing Violent Vid-6\n",
      "Processing NonViolent Vid-6\n",
      "Processing Violent Vid-7\n",
      "Processing NonViolent Vid-7\n",
      "Processing NonViolent Vid-8\n",
      "Processing Violent Vid-8\n",
      "Processing NonViolent Vid-9\n",
      "Processing Violent Vid-9\n",
      "Processing NonViolent Vid-10\n",
      "Processing Violent Vid-10\n",
      "Processing NonViolent Vid-11\n",
      "Processing Violent Vid-11\n",
      "Processing NonViolent Vid-12\n",
      "Processing Violent Vid-12\n",
      "Processing Violent Vid-13\n",
      "Processing NonViolent Vid-13\n",
      "Processing Violent Vid-14\n",
      "Processing NonViolent Vid-14\n",
      "Processing Violent Vid-15\n",
      "Processing NonViolent Vid-15\n",
      "Processing Violent Vid-16\n",
      "Processing NonViolent Vid-16\n",
      "Processing Violent Vid-17\n",
      "Processing Violent Vid-18\n",
      "Processing NonViolent Vid-17\n",
      "Processing Violent Vid-19\n",
      "Processing NonViolent Vid-18\n",
      "Processing Violent Vid-20\n",
      "Processing NonViolent Vid-19\n",
      "Processing NonViolent Vid-20\n",
      "Processing Violent Vid-21\n",
      "Processing NonViolent Vid-21\n",
      "Processing Violent Vid-22\n",
      "Processing NonViolent Vid-22\n",
      "Processing Violent Vid-23\n",
      "Processing NonViolent Vid-23\n",
      "Processing Violent Vid-24\n",
      "Processing NonViolent Vid-24\n",
      "Processing Violent Vid-25\n",
      "Processing NonViolent Vid-25\n",
      "Processing Violent Vid-26\n",
      "Processing NonViolent Vid-26\n",
      "Processing Violent Vid-27\n",
      "Processing NonViolent Vid-27\n",
      "Processing Violent Vid-28\n",
      "Processing NonViolent Vid-28\n",
      "Processing Violent Vid-29\n",
      "Processing NonViolent Vid-29\n",
      "Processing Violent Vid-30\n",
      "Processing NonViolent Vid-30\n",
      "Processing Violent Vid-31\n",
      "Processing Violent Vid-32\n",
      "Processing NonViolent Vid-31\n",
      "Processing Violent Vid-33\n",
      "Processing NonViolent Vid-32\n",
      "Processing Violent Vid-34\n",
      "Processing NonViolent Vid-33\n",
      "Processing NonViolent Vid-34\n",
      "Processing Violent Vid-35\n",
      "Processing NonViolent Vid-35\n",
      "Processing Violent Vid-36\n",
      "Processing NonViolent Vid-36\n",
      "Processing NonViolent Vid-37\n",
      "Processing Violent Vid-37\n",
      "Processing NonViolent Vid-38\n",
      "Processing NonViolent Vid-39\n",
      "Processing Violent Vid-38\n",
      "Processing Violent Vid-39\n",
      "Processing NonViolent Vid-40\n",
      "Processing Violent Vid-40\n",
      "Processing NonViolent Vid-41\n",
      "Processing Violent Vid-41\n",
      "Processing NonViolent Vid-42\n",
      "Processing NonViolent Vid-43\n",
      "Processing Violent Vid-42\n",
      "Processing NonViolent Vid-44\n",
      "Processing NonViolent Vid-45\n",
      "Processing Violent Vid-43\n",
      "Processing NonViolent Vid-46\n",
      "Processing Violent Vid-44\n",
      "Processing NonViolent Vid-47\n",
      "Processing Violent Vid-45\n",
      "Processing NonViolent Vid-48\n",
      "Processing Violent Vid-46\n",
      "Processing NonViolent Vid-49\n",
      "Processing Violent Vid-47\n",
      "Processing NonViolent Vid-50\n",
      "Processing Violent Vid-48\n",
      "Processing NonViolent Vid-51\n",
      "Processing NonViolent Vid-52\n",
      "Processing Violent Vid-49\n",
      "Processing NonViolent Vid-53\n",
      "Processing Violent Vid-50\n",
      "Processing NonViolent Vid-54\n",
      "Processing NonViolent Vid-55\n",
      "Processing Violent Vid-51\n",
      "Processing Violent Vid-52\n",
      "Processing NonViolent Vid-56\n",
      "Processing Violent Vid-53\n",
      "Processing NonViolent Vid-57\n",
      "Processing NonViolent Vid-58\n",
      "Processing Violent Vid-54\n",
      "Processing NonViolent Vid-59\n",
      "Processing Violent Vid-55\n",
      "Non-Violent Extracted\n",
      "Processing Violent Vid-56\n",
      "Processing Violent Vid-57\n",
      "Processing Violent Vid-58\n",
      "Processing Violent Vid-59\n",
      "Violent Extracted\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "t1 = Process(target=thread_1, args=())\n",
    "t2 = Process(target=thread_2, args=())\n",
    "\n",
    "t1.start() \n",
    "t2.start()\n",
    "\n",
    "t1.join()\n",
    "t2.join()\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='./Data'\n",
    "train_dir=os.path.join(base_dir,'Training')\n",
    "train_violent_dir =os.path.join(train_dir, 'V' )\n",
    "train_nonviolent_dir=os.path.join(train_dir,'NV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen= ImageDataGenerator(rescale=1./255, rotation_range=40,width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2,horizontal_flip=True, fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10280 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_dir,color_mode=\"rgb\", target_size = FRAME_SIZE,batch_size=20,classes=['NV','V'], class_mode='binary', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shobhits/Documents/Mini Project/CrimeDetection/env/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model= tf.keras.models.Sequential([\n",
    "       tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)),\n",
    "       tf.keras.layers.MaxPooling2D(2,2),\n",
    "       tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
    "       tf.keras.layers.MaxPooling2D(2,2),\n",
    "       tf.keras.layers.Conv2D(128,(3,3),activation='relu'),\n",
    "       tf.keras.layers.MaxPooling2D(2,2),\n",
    "       tf.keras.layers.Conv2D(128,(3,3),activation='relu'),\n",
    "       tf.keras.layers.MaxPooling2D(2,2),\n",
    "       tf.keras.layers.Dropout(0.5),\n",
    "       tf.keras.layers.Flatten(),\n",
    "       tf.keras.layers.Dense(512, activation='relu'),\n",
    "       tf.keras.layers.Dense(1,activation ='sigmoid')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shobhits/Documents/Mini Project/CrimeDetection/env/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 740ms/step - accuracy: 0.5098 - loss: 0.7030\n",
      "Epoch 2/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 716ms/step - accuracy: 0.6049 - loss: 0.6745\n",
      "Epoch 3/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 700ms/step - accuracy: 0.6936 - loss: 0.5739\n",
      "Epoch 4/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 711ms/step - accuracy: 0.7612 - loss: 0.5106\n",
      "Epoch 5/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 723ms/step - accuracy: 0.7687 - loss: 0.4867\n",
      "Epoch 6/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 717ms/step - accuracy: 0.7428 - loss: 0.5159\n",
      "Epoch 7/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 677ms/step - accuracy: 0.8172 - loss: 0.4275\n",
      "Epoch 8/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 737ms/step - accuracy: 0.7949 - loss: 0.4191\n",
      "Epoch 9/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 711ms/step - accuracy: 0.8222 - loss: 0.4185\n",
      "Epoch 10/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 680ms/step - accuracy: 0.8364 - loss: 0.3956\n",
      "Epoch 11/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 183ms/step - accuracy: 0.8124 - loss: 0.4029\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 16:30:18.188967: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/usr/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 822ms/step - accuracy: 0.8513 - loss: 0.3553\n",
      "Epoch 13/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 706ms/step - accuracy: 0.8535 - loss: 0.3564\n",
      "Epoch 14/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 704ms/step - accuracy: 0.8215 - loss: 0.4301\n",
      "Epoch 15/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 680ms/step - accuracy: 0.8589 - loss: 0.3399\n",
      "Epoch 16/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 696ms/step - accuracy: 0.8534 - loss: 0.3453\n",
      "Epoch 17/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 701ms/step - accuracy: 0.8765 - loss: 0.3090\n",
      "Epoch 18/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 692ms/step - accuracy: 0.8125 - loss: 0.4038\n",
      "Epoch 19/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 693ms/step - accuracy: 0.8480 - loss: 0.3337\n",
      "Epoch 20/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 705ms/step - accuracy: 0.8597 - loss: 0.3387\n",
      "Epoch 21/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 680ms/step - accuracy: 0.8718 - loss: 0.3189\n",
      "Epoch 22/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 177ms/step - accuracy: 0.7874 - loss: 0.4381 \n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 16:36:22.410893: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 730ms/step - accuracy: 0.8245 - loss: 0.3795\n",
      "Epoch 24/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 696ms/step - accuracy: 0.8651 - loss: 0.3325\n",
      "Epoch 25/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 704ms/step - accuracy: 0.8627 - loss: 0.3183\n",
      "Epoch 26/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 691ms/step - accuracy: 0.8692 - loss: 0.3165\n",
      "Epoch 27/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 700ms/step - accuracy: 0.8336 - loss: 0.3335\n",
      "Epoch 28/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 712ms/step - accuracy: 0.8780 - loss: 0.3025\n",
      "Epoch 29/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 725ms/step - accuracy: 0.8845 - loss: 0.3041\n",
      "Epoch 30/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 732ms/step - accuracy: 0.8881 - loss: 0.2614\n"
     ]
    }
   ],
   "source": [
    "model1=model.fit(train_generator,steps_per_epoch=50, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./1717240268.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "export_path_keras = \"./{}.h5\".format(int(t))\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
